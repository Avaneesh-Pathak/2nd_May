{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "Q2. What are the key challenges in anomaly detection?\n",
    "\n",
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "\n",
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Anomaly detection is a technique used in data analysis and machine learning to identify patterns or instances that deviate significantly from the norm or expected behavior within a dataset. Its purpose is to identify unusual or abnormal observations that may indicate potential fraud, errors, outliers, or other interesting events that require further investigation.\n",
    "\n",
    "Q2. There are several key challenges in anomaly detection, including:\n",
    "\n",
    "- Lack of labeled training data: Anomaly detection often deals with unlabeled datasets, making it challenging to apply traditional supervised learning techniques.\n",
    "- Imbalanced datasets: Anomalies are typically rare events compared to normal instances, leading to imbalanced datasets where the number of normal instances outweighs the anomalies. This can affect the performance of anomaly detection algorithms.\n",
    "- Understanding what constitutes normal behavior: Defining what is considered normal or abnormal can be subjective and context-dependent, making it challenging to establish a universal definition for anomalies.\n",
    "- Evolving anomalies: Anomalies can change over time, requiring anomaly detection systems to adapt and detect new types of anomalies.\n",
    "- High-dimensional data: Anomaly detection becomes more complex as the number of dimensions in the dataset increases, leading to the curse of dimensionality and increased computational complexity.\n",
    "\n",
    "Q3. Unsupervised anomaly detection does not rely on labeled data during the training phase. It aims to identify anomalies based solely on the characteristics and patterns present in the data itself. In contrast, supervised anomaly detection requires labeled data, where both normal and anomalous instances are known during training. The supervised approach learns from the labeled examples to classify future instances as normal or anomalous.\n",
    "\n",
    "Q4. The main categories of anomaly detection algorithms include:\n",
    "\n",
    "- Statistical methods: These algorithms assume that normal data points follow a certain statistical distribution, and anomalies are considered as deviations from this distribution. Examples include Gaussian distribution-based models, such as the Z-score or the modified Z-score method.\n",
    "\n",
    "- Machine learning methods: These algorithms utilize various machine learning techniques to learn patterns from the data and identify anomalies. Examples include clustering-based methods, classification-based methods, and ensemble methods like Isolation Forest and Local Outlier Factor (LOF).\n",
    "\n",
    "- Proximity-based methods: These algorithms measure the distance or similarity between data points and identify anomalies based on their distance from the rest of the data. Examples include k-nearest neighbors (KNN) and distance-based outliers (DBO).\n",
    "\n",
    "- Information-theoretic methods: These algorithms assess the information content or complexity of data points to identify anomalies. Examples include entropy-based methods and Minimum Description Length (MDL) principle.\n",
    "\n",
    "Q5. Distance-based anomaly detection methods make several assumptions, including:\n",
    "\n",
    "- Anomalies are located far away from normal instances in the feature space.\n",
    "- Normal instances are dense and form clusters, while anomalies are isolated or far from any cluster.\n",
    "- The distance metric used is appropriate for measuring the dissimilarity between data points.\n",
    "- The number of dimensions in the dataset is not excessively high, as the curse of dimensionality can affect distance-based calculations.\n",
    "\n",
    "Q6. The Local Outlier Factor (LOF) algorithm computes anomaly scores as follows:\n",
    "\n",
    "1. Compute the k-distance for each data point, which is the distance to its k-th nearest neighbor.\n",
    "2. For each data point, calculate the reachability distance for its k-th nearest neighbor.\n",
    "3. Compute the Local Reachability Density (LRD) for each data point, which is the inverse of the average reachability distance of its k nearest neighbors.\n",
    "4. Calculate the Local Outlier Factor (LOF) for each data point, which represents the degree to which a point differs from its neighbors in terms of density. LOF is the average ratio of the LRD of a data point to the LRD of its k nearest neighbors.\n",
    "5. The anomaly score for each data point is the LOF value. Higher LOF values indicate more anomalous points.\n",
    "\n",
    "Q7. The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "- Number of trees: It determines the number of isolation trees to be built. Increasing the number of trees generally improves the accuracy but also increases the computational cost.\n",
    "- Subsample size: It controls the size of the random subsets of the data used to build each isolation tree. A smaller subsample size can lead to more diversity among the trees but may increase the chance of overfitting.\n",
    "- Contamination: It represents the expected proportion of anomalies in the data. It helps in scaling the anomaly scores to the range [0, 1].\n",
    "\n",
    "Q8. In KNN with K=10, if a data point has only 2 neighbors of the same class within a radius of 0.5, its anomaly score would be relatively high. Since the majority of the neighbors within the KNN search radius are not of the same class, it suggests that the data point deviates from the normal behavior and may be considered an anomaly.\n",
    "\n",
    "Q9. The average path length in an Isolation Forest represents the average number of edges traversed for a data point to reach an external node in the forest. In this case, if a data point has an average path length of 5.0 compared to the average path length of the trees, it suggests that the data point reaches external nodes faster than the average. As a result, its anomaly score would be relatively low, indicating that it is less likely to be an anomaly compared to points with longer average path lengths. However, the exact calculation of the anomaly score also considers the expected average path length, the number of trees, and the contamination parameter, so a precise score cannot be determined without additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
